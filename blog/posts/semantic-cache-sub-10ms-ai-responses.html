<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Sub-10ms AI Responses Without Calling the LLM &mdash; Adri&agrave; Cidre</title>
  <meta name="description" content="Users ask similar questions in different words. Semantic caching with pgvector turns repeated intent into instant answers &mdash; no LLM call, no embedding, no retrieval pipeline.">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://adriacidre.com/blog/posts/semantic-cache-sub-10ms-ai-responses.html">
  <meta property="og:title" content="Sub-10ms AI Responses Without Calling the LLM">
  <meta property="og:description" content="Users ask similar questions in different words. Semantic caching with pgvector turns repeated intent into instant answers &mdash; no LLM call, no embedding, no retrieval pipeline.">
  <meta property="og:image" content="https://adriacidre.com/blog/images/og-default.jpg">
  <meta property="og:site_name" content="Adri&agrave; Cidre">
  <meta property="article:published_time" content="2026-03-05">
  <meta property="article:author" content="Adri&agrave; Cidre">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Sub-10ms AI Responses Without Calling the LLM">
  <meta name="twitter:description" content="Users ask similar questions in different words. Semantic caching with pgvector turns repeated intent into instant answers &mdash; no LLM call, no embedding, no retrieval pipeline.">
  <meta name="twitter:image" content="https://adriacidre.com/blog/images/og-default.jpg">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@300;600&family=Source+Code+Pro:wght@400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark-dimmed.min.css">
  <link rel="stylesheet" href="../css/blog.css">
</head>
<body>
  <div class="blog-wrapper">
    <div class="blog-card">
      <header class="blog-header">
        <div class="blog-header-name"><a href="/">Adri&agrave; Cidre</a></div>
        <nav>
          <ul class="blog-header-nav">
            <li><a href="/">Home</a></li>
            <li><a href="/blog/">Blog</a></li>
          </ul>
        </nav>
      </header>

      <div class="blog-content">
        <div class="post-meta">
          <span class="post-meta-date">March 5, 2026</span>
          <div class="post-meta-tags">
            <span class="post-meta-tag">semantic-caching</span>
            <span class="post-meta-tag">llm-caching</span>
            <span class="post-meta-tag">inference-cost-reduction</span>
            <span class="post-meta-tag">latency-optimization</span>
            <span class="post-meta-tag">embedding-based-caching</span>
            <span class="post-meta-tag">rag-pipeline</span>
            <span class="post-meta-tag">pgvector</span>
          </div>
        </div>

        <h1>Sub-10ms AI Responses Without Calling the LLM</h1>

        <p>A user asks &ldquo;How do I request vacation?&rdquo; Your Q&amp;A assistant retrieves documents, builds a prompt, calls the LLM, streams the response. Two seconds, maybe three. The next user asks &ldquo;What&rsquo;s the time-off process?&rdquo; Same answer. Same two seconds. Same cost.</p>

        <p>Exact-match caching doesn&rsquo;t help &mdash; the strings are different. But the <em>intent</em> is identical. I needed a cache that understood meaning, not just characters.</p>

        <h2>The naive approach: exact-match caching</h2>

        <p>The first thing everyone tries is hashing the question and looking it up in a key-value store. If the hash matches, return the cached answer. Simple, fast, and almost completely useless.</p>

        <p>Real users don&rsquo;t ask questions the same way twice. &ldquo;How do I request PTO?&rdquo;, &ldquo;Where do I submit a vacation request?&rdquo;, &ldquo;Time off &mdash; how?&rdquo; &mdash; all the same question, zero cache hits. I measured the hit rate on exact-match caching against real traffic: under 3%. Not worth the code.</p>

        <h2>Embedding the question</h2>

        <p>The fix is to compare questions by meaning instead of characters. Every incoming question gets turned into a vector embedding &mdash; a list of numbers that captures its semantic content. Then instead of exact string comparison, you do a nearest-neighbor search against all previously cached question-answer pairs.</p>

<pre><code class="language-go">// LookupSemanticCache checks if a semantically similar question
// has already been answered. Returns the cached answer if the
// similarity exceeds the threshold.
func LookupSemanticCache(
    ctx context.Context,
    db *pgxpool.Pool,
    questionEmbedding []float32,
    tenantID string,
    threshold float64,
) (*CachedAnswer, error) {
    var answer CachedAnswer
    err := db.QueryRow(ctx, `
        SELECT
            answer_text,
            sources_referenced,
            1 - (question_embedding <=> $1::vector) AS similarity
        FROM qa_cache
        WHERE tenant_id = $2
        ORDER BY question_embedding <=> $1::vector
        LIMIT 1
    `, pgvector.NewVector(questionEmbedding), tenantID).Scan(
        &answer.Text,
        &answer.SourcesReferenced,
        &answer.Similarity,
    )
    if err != nil {
        return nil, err
    }
    if answer.Similarity < threshold {
        return nil, nil // no close enough match
    }
    return &answer, nil
}</code></pre>

        <p>The <code>&lt;=&gt;</code> operator is pgvector&rsquo;s cosine distance. Lower distance means higher similarity. The query finds the closest cached question to the incoming one, and if it&rsquo;s close enough, returns the cached answer.</p>

        <h2>Threshold tuning: the number that matters</h2>

        <p>I use a cosine distance threshold of 0.15 (which corresponds to a similarity of 0.85). This number took some tuning to get right, and it&rsquo;s the single most important parameter in the whole system.</p>

        <p>The tradeoff is simple:</p>

        <ul>
          <li><strong>Too low (more permissive)</strong> &mdash; the cache returns answers for questions that are close but not close enough. &ldquo;How do I request vacation?&rdquo; matches &ldquo;How do I cancel my vacation?&rdquo; Same topic, opposite intent. Wrong cached answer served with full confidence.</li>
          <li><strong>Too high (more strict)</strong> &mdash; the cache almost never hits. You&rsquo;re paying for embeddings on every question but rarely getting the benefit.</li>
        </ul>

        <p>I arrived at 0.85 by logging every cache lookup for two weeks &mdash; the question, the nearest match, the similarity score, and whether the cached answer was actually correct for the new question. 0.85 gave me a false-positive rate under 2% with a cache hit rate around 35% of all incoming questions.</p>

<pre><code class="language-go">const (
    // SimilarityThreshold is the minimum cosine similarity required
    // for a cache hit. Tuned against production traffic to balance
    // hit rate (~35%) against false positives (<2%).
    SimilarityThreshold = 0.85
)</code></pre>

        <h2>The schema</h2>

        <p>The cache lives in PostgreSQL with the pgvector extension. Each entry stores the question embedding, the generated answer, and metadata about which source documents were used to produce it.</p>

<pre><code class="language-sql">CREATE TABLE qa_cache (
    id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id       TEXT NOT NULL,
    question_text   TEXT NOT NULL,
    question_embedding vector(1536) NOT NULL,
    answer_text     TEXT NOT NULL,
    sources_referenced TEXT[] NOT NULL DEFAULT '{}',
    language        TEXT NOT NULL DEFAULT 'en',
    hit_count       INTEGER NOT NULL DEFAULT 0,
    created_at      TIMESTAMPTZ NOT NULL DEFAULT now(),
    last_hit_at     TIMESTAMPTZ,
    invalidated_at  TIMESTAMPTZ
);

-- HNSW index for fast approximate nearest-neighbor search.
-- ef_construction=128 and m=16 give good recall at this scale.
CREATE INDEX idx_qa_cache_embedding
    ON qa_cache
    USING hnsw (question_embedding vector_cosine_ops)
    WITH (m = 16, ef_construction = 128);

-- Tenant-scoped lookups are the common path.
CREATE INDEX idx_qa_cache_tenant
    ON qa_cache (tenant_id)
    WHERE invalidated_at IS NULL;</code></pre>

        <p>Two things to note:</p>

        <ul>
          <li><strong>Tenant scoping.</strong> Every query filters by <code>tenant_id</code>. Each organization only searches its own cache. This matters because the same question can have different correct answers depending on the organization&rsquo;s documents. &ldquo;What&rsquo;s the vacation policy?&rdquo; has a different answer for every company.</li>
          <li><strong>HNSW index.</strong> This is what makes the lookup fast. HNSW (Hierarchical Navigable Small World) is an approximate nearest-neighbor algorithm that trades a small amount of accuracy for massive speed gains. At 100K cached entries, a lookup takes single-digit milliseconds.</li>
        </ul>

        <h2>Soft invalidation</h2>

        <p>The hard part of any cache is invalidation. When a source document changes, which cached answers are now stale?</p>

        <p>The naive approach is to nuke the entire cache whenever any document changes. This works but it&rsquo;s wasteful &mdash; if you update the vacation policy document, you don&rsquo;t need to invalidate cached answers about the expense report process.</p>

        <p>The <code>sources_referenced</code> array makes surgical invalidation possible. When a document changes, you only invalidate cache entries that actually cited that document:</p>

<pre><code class="language-go">// InvalidateBySource marks cache entries as stale when a source
// document they reference has been updated. Only affects entries
// that actually cited the changed document.
func InvalidateBySource(
    ctx context.Context,
    db *pgxpool.Pool,
    tenantID string,
    sourceID string,
) (int64, error) {
    tag, err := db.Exec(ctx, `
        UPDATE qa_cache
        SET invalidated_at = now()
        WHERE tenant_id = $1
          AND $2 = ANY(sources_referenced)
          AND invalidated_at IS NULL
    `, tenantID, sourceID)
    if err != nil {
        return 0, err
    }
    return tag.RowsAffected(), nil
}</code></pre>

        <p>When the vacation policy document gets updated, only the cached Q&amp;A pairs that referenced it get invalidated. Everything else stays warm. In practice, a single document update invalidates 5&ndash;15% of a tenant&rsquo;s cache entries, not 100%.</p>

        <p>Invalidated entries aren&rsquo;t deleted &mdash; they&rsquo;re marked with a timestamp. The lookup query filters them out with <code>WHERE invalidated_at IS NULL</code>. This makes invalidation a fast UPDATE rather than a DELETE, and you keep the history for debugging.</p>

        <h2>The full lookup flow</h2>

        <p>Putting it all together, the cache sits in front of the full RAG pipeline:</p>

<pre><code class="language-plaintext">User question
     |
     v
Embed question (one API call, ~50ms)
     |
     v
Search cache (pgvector HNSW, ~5ms)
     |
     +---> Cache HIT (similarity >= 0.85)
     |         |
     |         v
     |     Return cached answer (~0ms)
     |     Total: ~55ms
     |
     +---> Cache MISS
               |
               v
           Full RAG pipeline:
           Retrieve docs + Build prompt + LLM call
           Total: 2000-3000ms
               |
               v
           Store in cache for next time</code></pre>

        <p>A cache hit skips the document retrieval, prompt construction, and LLM call entirely. The only cost is the embedding of the incoming question (which you need anyway for the similarity search) plus the vector lookup. That&rsquo;s ~55ms end-to-end versus 2&ndash;3 seconds for the full pipeline.</p>

        <p>The &ldquo;sub-10ms&rdquo; in the title refers to the cache lookup itself, not the embedding step. If you pre-compute embeddings (which some architectures allow), the entire response is sub-10ms.</p>

        <h2>The technical names I didn&rsquo;t know</h2>

        <p>After building this, I found the established terminology:</p>

        <ul>
          <li><strong>Semantic caching</strong> &mdash; caching by meaning rather than exact key. The term comes from database research (semantic query caching) but maps directly to what I built for LLM responses.</li>
          <li><strong>Cache coherence</strong> &mdash; keeping cached data consistent with its source of truth. In distributed systems this is about CPU cache lines and memory barriers. Here it&rsquo;s about invalidating cached AI answers when their source documents change.</li>
          <li><strong>Soft invalidation</strong> &mdash; marking entries as stale rather than deleting them. Common in CDN and browser caching. The &ldquo;stale-while-revalidate&rdquo; pattern is a cousin of what I&rsquo;m doing here.</li>
        </ul>

        <p>These are classic caching patterns from distributed systems &mdash; I just applied them to AI inference instead of web pages or database queries.</p>

        <h2>What doesn&rsquo;t work</h2>

        <ul>
          <li><strong>Context-dependent questions.</strong> &ldquo;What permissions do I have?&rdquo; looks the same regardless of who&rsquo;s asking, but the answer depends on the user&rsquo;s role. Semantic similarity can&rsquo;t capture this &mdash; the question embeddings are identical. I handle this by including role information in the cache key (effectively partitioning the cache by role), but it reduces the hit rate.</li>
          <li><strong>Evolving answers.</strong> Some questions have answers that change frequently. Caching them means serving stale information until invalidation fires. The <code>sources_referenced</code> tracking helps, but only if the answer was actually generated from those sources. If the LLM synthesized something from general knowledge, there&rsquo;s no source to track.</li>
          <li><strong>Multilingual overlap.</strong> The same question in different languages produces different embeddings. &ldquo;How do I request vacation?&rdquo; and &ldquo;&iquest;C&oacute;mo solicito vacaciones?&rdquo; are semantically identical but won&rsquo;t match. I store a <code>language</code> field and only match within the same language, which means separate cache pools per language.</li>
          <li><strong>The similarity threshold is a blunt instrument.</strong> 0.85 works on average, but some question clusters are tighter (0.90 would be fine) and others are wider (0.80 would be better). A per-topic threshold would be more accurate but dramatically more complex.</li>
        </ul>

        <h2>Cost impact</h2>

        <p>Every cache hit saves:</p>

        <ul>
          <li>One LLM call (the big one &mdash; this is where most of the cost and latency lives)</li>
          <li>One document retrieval operation</li>
          <li>One prompt construction step</li>
        </ul>

        <p>At a 35% hit rate across all incoming questions, that&rsquo;s roughly a third of your LLM costs eliminated. The embedding cost for the cache lookup is a rounding error compared to the LLM call it replaces. And the user experience improvement is dramatic &mdash; going from a 2-second response to a sub-100ms response for over a third of questions makes the system feel qualitatively different.</p>

        <p>The cache pays for itself almost immediately. The pgvector index, the storage, the embedding calls for lookups &mdash; all of it combined costs less than the LLM calls you&rsquo;re avoiding.</p>

        <hr>

        <nav class="post-nav">
          <a href="/blog/">&larr; Back to blog</a>
          <span class="post-nav-spacer"></span>
        </nav>
      </div>

      <footer class="blog-footer">
        <p>&copy; Adri&agrave; Cidre &middot; <a href="/">Home</a></p>
      </footer>
    </div>
  </div>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</body>
</html>
